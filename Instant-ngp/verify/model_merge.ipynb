{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca34a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 变换位置编码后的验证程序\n",
    "import torch\n",
    "import numpy as np\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from matplotlib.widgets import Slider\n",
    "from skimage import measure\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import io\n",
    "\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, output_ch=2, skips=[4], use_viewdirs=True):\n",
    "        \"\"\"\n",
    "        D: 深度，多少层网络\n",
    "        W: 网络内的channel 宽度\n",
    "        input_ch: xyz的宽度\n",
    "        input_ch_views: direction的宽度\n",
    "        output_ch: 这个参数尽在 use_viewdirs=False的时候会被使用\n",
    "        skips: 类似resnet的残差连接，表明在第几层进行连接\n",
    "        use_viewdirs:\n",
    "\n",
    "        网络输入已经被位置编码后的参数，输入为[64*bs,90]，输出为[64*bs，2]，一位是体积密度，一位是后向散射系数\n",
    "        \"\"\"\n",
    "        super(NeRF, self).__init__()\n",
    "        self.D = D\n",
    "        self.W = W\n",
    "        self.input_ch = input_ch\n",
    "        self.input_ch_views = input_ch_views\n",
    "        self.skips = skips\n",
    "        self.use_viewdirs = use_viewdirs\n",
    "\n",
    "        # 神经网络,MLP\n",
    "        # 3D的空间坐标进入的网络\n",
    "        # 这个跳跃连接层是直接拼接，不是resnet的那种相加\n",
    "        self.pts_linears = nn.ModuleList(\n",
    "            [nn.Linear(input_ch, W)] + [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + input_ch, W) for i in\n",
    "                                        range(D - 1)])\n",
    "\n",
    "        # 这里channel削减一半 128\n",
    "        ### Implementation according to the official code release (https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L104-L105)\n",
    "        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W // 2)])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        if use_viewdirs:\n",
    "            # 特征\n",
    "            self.feature_linear = nn.Linear(W, W)\n",
    "            # 体积密度,一个值\n",
    "            self.alpha_linear = nn.Linear(W, 1)\n",
    "            # 后向散射系数，一个值\n",
    "            self.rho_linear = nn.Linear(W // 2, 1)\n",
    "        else:\n",
    "            self.output_linear = nn.Linear(W, output_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x [bs*64, 90]\n",
    "        # input_pts [bs*64, 63]\n",
    "        # input_views [bs*64,27]\n",
    "        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-1)\n",
    "\n",
    "        h = input_pts\n",
    "\n",
    "        for i, l in enumerate(self.pts_linears):\n",
    "\n",
    "            h = self.pts_linears[i](h)\n",
    "            h = F.relu(h)\n",
    "            # 第四层后相加\n",
    "            if i in self.skips:\n",
    "                h = torch.cat([input_pts, h], -1)\n",
    "\n",
    "        if self.use_viewdirs:\n",
    "            # alpha只与xyz有关\n",
    "            alpha = self.alpha_linear(h)\n",
    "            feature = self.feature_linear(h)\n",
    "            # rho与xyz和d都有关\n",
    "            h = torch.cat([feature, input_views], -1)\n",
    "\n",
    "            for i, l in enumerate(self.views_linears):\n",
    "                h = self.views_linears[i](h)\n",
    "                h = F.relu(h)\n",
    "\n",
    "            sigma = self.rho_linear(h)\n",
    "            alpha = self.relu(alpha) \n",
    "            # sigma = 100*self.sigmoid(sigma)\n",
    "            sigma = self.relu(sigma) \n",
    "            \n",
    "            outputs = torch.cat([alpha, sigma], -1)\n",
    "        else:\n",
    "            outputs = self.output_linear(h)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def batchrender(omega,LOS,model,doppler_num):\n",
    "    '''\n",
    "    omega为一个[bs,3]变量，指向旋转轴方向，模值为角速度\n",
    "    LOS为一个[bs,3]变量，方向为视线方向指向物体，模值为1\n",
    "    model是nerf模型，将一个已经进行位置编码后的位置和视线向量输入进model,可以返回这个位置的体积密度和散射系数\n",
    "    doppler_num为一个[bs]变量，确定了渲染后光线所在的位置\n",
    "    '''\n",
    "    # 确定回波波长\n",
    "    fc = torch.tensor([9.7e9]).to(device)\n",
    "    c = torch.tensor([299792458]).to(device)\n",
    "    lambda0 = c/fc\n",
    "    # 确定网格参数\n",
    "    distance_max = 0.6\n",
    "    distance_min = -0.6\n",
    "    distance_gap = 100\n",
    "    doppler_max = 0.15\n",
    "    doppler_min = -0.15\n",
    "    doppler_gap = 100\n",
    "    n_max = 0.60\n",
    "    n_min = -0.60\n",
    "    n_gap = 120\n",
    "    # distance_max = 0.582\n",
    "    # distance_min = -0.582\n",
    "    # distance_gap = 97\n",
    "    # doppler_max = 0.1575\n",
    "    # doppler_min = -0.1575\n",
    "    # doppler_gap = 105\n",
    "    # n_max = 0.60\n",
    "    # n_min = -0.60\n",
    "    # n_gap = 120\n",
    "    # 确定输入batch_size\n",
    "    batch_size,len = omega.shape\n",
    "    # 确定每个batch_size输入的投影平面\n",
    "    omega_norm = torch.linalg.norm(omega,dim = 1)\n",
    "    omega_normlize = omega/omega_norm.unsqueeze(1)\n",
    "    Doppler_vector = torch.cross(LOS,omega,dim=1)\n",
    "    LOSomega_sin_angel = torch.linalg.norm(Doppler_vector,dim=1)/(torch.linalg.norm(omega,dim=1)*torch.linalg.norm(LOS,dim=1))\n",
    "    Doppler_vector = Doppler_vector/torch.linalg.norm(Doppler_vector,dim = 1).unsqueeze(1)\n",
    "    # 绘制投影坐标\n",
    "    distance = torch.linspace(distance_min,distance_max,distance_gap).to(device)\n",
    "    distance = distance.repeat(batch_size,1)\n",
    "    distance_delta = torch.tensor((distance_max-distance_min)/distance_gap).to(device)\n",
    "    doppler = torch.linspace(doppler_min,doppler_max,doppler_gap).repeat(batch_size,1).to(device)\n",
    "    doppler = doppler*4/LOSomega_sin_angel.unsqueeze(1)\n",
    "    distance_map = distance.unsqueeze(2)*LOS.unsqueeze(1)\n",
    "    doppler_map = doppler.unsqueeze(2)*Doppler_vector.unsqueeze(1)\n",
    "    # 确定投影平面法向量\n",
    "    n = torch.cross(LOS,Doppler_vector,dim=1)\n",
    "    n = n/torch.linalg.norm(n,dim = 1).unsqueeze(1)\n",
    "    # 对投影平面法向量进行随机采样\n",
    "    n_array = torch.linspace(n_min,n_max,n_gap+1).to(device)\n",
    "    n_array = n_array.repeat(batch_size,distance_gap,1)\n",
    "    # 非随机采样\n",
    "    # n_random_array = n_array[:,:,0:-1] + (n_array[:,:,1:] - n_array[:,:,0:-1])*torch.ones(batch_size,distance_gap,n_gap).to(device)*0.5\n",
    "    # # 随机采样\n",
    "    n_random_array = n_array[:,:,0:-1] + (n_array[:,:,1:] - n_array[:,:,0:-1])*torch.rand(batch_size,distance_gap,n_gap).to(device)\n",
    "    n_random_map = n_random_array.unsqueeze(3)*n.unsqueeze(1).unsqueeze(2)\n",
    "    # 计算不同随机法向量之间的间隔\n",
    "    start_n = n.unsqueeze(1).unsqueeze(2)*torch.tensor(n_min).float().to(device)\n",
    "    start_n = start_n * torch.ones(batch_size,distance_gap,1,3).to(device)\n",
    "    n_random_map_temp = torch.cat((start_n,n_random_map),dim=2)\n",
    "    n_delta = torch.norm(n_random_map_temp[:,:,0:-1,:]-n_random_map,dim=3)\n",
    "\n",
    "    # 计算所有需要输入网络的坐标\n",
    "    code_flag = 1\n",
    "    if code_flag == 1:\n",
    "        xyz = doppler_map[torch.arange(batch_size),doppler_num,:].unsqueeze(1).unsqueeze(2) + distance_map.unsqueeze(2) + n_random_map\n",
    "        xyz_coding = positon_code_xyz(xyz)\n",
    "        LOS_coding = position_code_LOS(LOS)\n",
    "        LOS_coding = ((LOS_coding.unsqueeze(1).unsqueeze(2))*torch.ones(batch_size,distance_gap,n_gap,27).to(device)).view(-1,27)\n",
    "        xyzLOS_coding = torch.cat((xyz_coding,LOS_coding),dim=1)\n",
    "    else:\n",
    "        xyz = doppler_map[torch.arange(batch_size),doppler_num,:].unsqueeze(1).unsqueeze(2) + distance_map.unsqueeze(2) + n_random_map\n",
    "        xyz_coding = xyz.view(-1,3)\n",
    "        LOS_coding = ((LOS.unsqueeze(1).unsqueeze(2))*torch.ones(batch_size,distance_gap,n_gap,3).to(device)).view(-1,3)\n",
    "        print(LOS_coding)\n",
    "        xyzLOS_coding = torch.cat((xyz_coding,LOS_coding),dim=1)\n",
    "    output = model(xyzLOS_coding)\n",
    "    output = output.view(batch_size,distance_gap,n_gap,2)\n",
    "    render_equaltion = 1\n",
    "    if render_equaltion == 0:\n",
    "        Ti = torch.cumprod(torch.exp(-output[:,:,:,0]*distance_delta),dim=1)\n",
    "        distance_profile = torch.sum(output[:,:,:,0]*(1-torch.exp(-output[:,:,:,1]*n_delta))*Ti,dim=2)\n",
    "    elif render_equaltion == 1:\n",
    "        Ti = torch.cumprod(torch.exp(-output[:,:,:,0]**2*distance_delta),dim=1)\n",
    "        temp = output[:,:,:,0]*output[:,:,:,1]*n_delta*Ti\n",
    "        distance_profile = torch.sum(output[:,:,:,0]*output[:,:,:,1]*n_delta*Ti,dim=2)\n",
    "    elif render_equaltion == 2:\n",
    "        Ti = torch.cumprod(torch.exp(-output[:,:,:,0]*distance_delta),dim=1)\n",
    "        # print(Ti[:,0,:])\n",
    "        # 将Ti的第1维首增加一个1，并去除最后一维，方便计算\n",
    "        Ti = torch.cat((torch.ones(batch_size,1,n_gap).to(device),Ti),dim=1)[:,:-1,:]\n",
    "        # 计算alpha_i\n",
    "        alphai = 1-torch.exp(-output[:,:,:,0]*distance_delta)\n",
    "        temp = alphai*output[:,:,:,1]*n_delta*Ti\n",
    "        distance_profile = torch.sum(temp,dim=2)\n",
    "    elif render_equaltion == 3:\n",
    "        # Ranerf的累积规则\n",
    "        Ti = torch.cumprod(torch.exp(-output[:,:,:,0]*distance_delta),dim=2)\n",
    "        Ti = torch.cat((torch.ones(batch_size,distance_gap,1).to(device),Ti),dim=2)[:,:,:-1]\n",
    "        alphai = 1-torch.exp(-output[:,:,:,0]*n_delta)\n",
    "        temp = alphai*output[:,:,:,1]*n_delta*Ti\n",
    "        distance_profile = torch.sum(alphai*output[:,:,:,1]*n_delta*Ti,dim=2)    \n",
    "    return distance_profile,temp[:,:,:]\n",
    "    # return distance_profile,output[:,:,:,0]\n",
    "\n",
    "\n",
    "def positon_code_xyz(xyz):\n",
    "    code_len = 10\n",
    "    batch_size,distance,n,dimension = xyz.shape\n",
    "    xyz = xyz.view(-1,dimension)\n",
    "    xyz = xyz\n",
    "    position_coding = torch.zeros_like(xyz).to(device)\n",
    "    position_coding = position_coding.repeat(1,code_len*2)\n",
    "    div_term = 2 ** torch.arange(0,code_len,step=1).to(device)\n",
    "    position_coding[:,0::2] = torch.sin((xyz.unsqueeze(1) * math.pi * div_term.unsqueeze(1).unsqueeze(0)).view(batch_size*distance*n,-1))\n",
    "    position_coding[:,1::2] = torch.cos((xyz.unsqueeze(1) * math.pi * div_term.unsqueeze(1).unsqueeze(0)).view(batch_size*distance*n,-1))\n",
    "    position_coding = torch.cat((xyz,position_coding),dim=1)\n",
    "    \n",
    "    return position_coding\n",
    "\n",
    "def position_code_LOS(LOS):\n",
    "    code_len = 4\n",
    "    batch_size,dimension = LOS.shape\n",
    "    position_coding = torch.zeros_like(LOS).to(device)\n",
    "    position_coding = position_coding.repeat(1,code_len*2)\n",
    "    div_term = 2 ** torch.arange(0,code_len,step=1).to(device)\n",
    "    position_coding[:,0::2] = torch.sin((LOS.unsqueeze(1) * div_term.unsqueeze(1).unsqueeze(0)).view(batch_size,-1))\n",
    "    position_coding[:,1::2] = torch.cos((LOS.unsqueeze(1) * div_term.unsqueeze(1).unsqueeze(0)).view(batch_size,-1))\n",
    "    position_coding = torch.cat((LOS,position_coding),dim=1)\n",
    "\n",
    "    return position_coding\n",
    "\n",
    "def picture_sample(images,LOS_dirs,omegas,batch_size,image_hight = 100,image_width = 100, image_num = 30,temp_num = None):\n",
    "    # temp_num = random.sample(range(image_num),1)\n",
    "    # temp_num = [54]\n",
    "    print(temp_num)\n",
    "    temp = [t*image_hight for t in temp_num] + np.arange(100)\n",
    "    data_num = [x//image_hight for x in temp]\n",
    "    doppler_numbers = [x % image_hight for x in temp]\n",
    "    \n",
    "    LOS_dirs_batch = [LOS_dirs[x] for x in data_num]\n",
    "    omegas_batch = [omegas[x] for x in data_num]\n",
    "    range_profile_batch = [images[x][y,:] for x,y in zip(data_num,doppler_numbers)]\n",
    "\n",
    "\n",
    "    omegas_batch_tensor = torch.stack(omegas_batch).to(device)\n",
    "    LOS_dirs_batch_tensor = torch.stack(LOS_dirs_batch).to(device)\n",
    "    range_profile_batch_tensor = torch.stack(range_profile_batch).to(device)\n",
    "    doppler_profil_num_tensor = torch.tensor(doppler_numbers).long().to(device)\n",
    "\n",
    "    # range_image = range_profile_batch_tensor.detach().cpu()\n",
    "    # plt.imshow(range_image)\n",
    "    # plt.show()\n",
    "    \n",
    "    return omegas_batch_tensor,LOS_dirs_batch_tensor,range_profile_batch_tensor,doppler_profil_num_tensor\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    # 分割字符串中的数字并将它们转换为整数\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def loaddata(folder_path):\n",
    "    '''\n",
    "    输入文件夹路径，输出数据集\n",
    "    '''\n",
    "    # 获取文件夹中的所有文件和子文件夹\n",
    "    items = os.listdir(folder_path)\n",
    "    # 过滤出所有文件（排除子文件夹）\n",
    "    files = [item for item in items if os.path.isfile(os.path.join(folder_path, item)) and item.endswith('.npz')]\n",
    "    files_sorted = sorted(files, key=natural_sort_key)\n",
    "    # files_sorted = random.sample(files_sorted, 16)\n",
    "    #载入数据\n",
    "    images = []\n",
    "    LOS_dirs = []\n",
    "    omegas = []\n",
    "    for file in files_sorted:\n",
    "        full_path = folder_path+\"/\"+file\n",
    "        data = np.load(full_path)\n",
    "        image = torch.from_numpy(data['image']).to(device)\n",
    "        LOS_dir = torch.from_numpy(data['LOS']).to(device)\n",
    "        omega = torch.from_numpy(data['rotation_axis']).to(device)\n",
    "        images.append(image)\n",
    "        LOS_dirs.append(LOS_dir)\n",
    "        omegas.append(omega)\n",
    "\n",
    "    # 可视化LOS_dirs\n",
    "    LOS_dirs = torch.stack(LOS_dirs)\n",
    "    print(LOS_dirs.shape)\n",
    "    # 在三维空间中表示LOS_dirs\n",
    "    fig = go.Figure(data=[go.Scatter3d(x=LOS_dirs[:, 0].cpu().numpy(),\n",
    "                                        y=LOS_dirs[:, 1].cpu().numpy(),\n",
    "                                        z=LOS_dirs[:, 2].cpu().numpy(),\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(size=2, color='blue'))])\n",
    "    fig.update_layout(scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),\n",
    "                      title='LOS Directions in 3D Space')\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return images,LOS_dirs,omegas\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "modelname = 'experiment210'\n",
    "\n",
    "model = NeRF(input_ch = 63, input_ch_views = 27, use_viewdirs = True).to(device)\n",
    "model.load_state_dict(torch.load('/DATA/disk1/asteroid/asteroid_inverse/Instant-ngp/model/'+ modelname +'/model_state_dict.pth'))\n",
    "model.eval()\n",
    "\n",
    "omega_real = math.pi/900\n",
    "\n",
    "folder_path = '/DATA/disk1/asteroid/asteroid_inverse/ImageGen/3dmodel/XXX/XXX_dilate_real_image_13.8du'\n",
    "# folder_path = '/DATA/disk1/asteroid/asteroid_inverse/Instant-ngp/new_dataset/sys_data/contactball_rot90'\n",
    "\n",
    "images,LOS_dirs,omegas = loaddata(folder_path)\n",
    "\n",
    "# 获得list类型数据omegas的长度\n",
    "image_num = len(images)\n",
    "\n",
    "# omegas_batch_tensor,LOS_dirs_batch_tensor,range_profile_batch_tensor,doppler_profil_num_tensor = picture_sample(images,LOS_dirs,omegas,batch_size = 40,image_num=image_num)\n",
    "# distance_profile_batch,output = batchrender(omegas_batch_tensor*omega_real,LOS_dirs_batch_tensor,model,doppler_profil_num_tensor)\n",
    "\n",
    "# # print(LOS_dirs)\n",
    "# # print(LOS_dirs_batch_tensor)\n",
    "\n",
    "# range_image1 = distance_profile_batch.detach().cpu()\n",
    "# # plt.figure()\n",
    "# # plt.imshow(range_image1)\n",
    "# # plt.colorbar()\n",
    "\n",
    "\n",
    "# range_image2 = range_profile_batch_tensor.detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "# ray_distribution = output.detach().cpu().numpy()\n",
    "# # ray_distribution_one = ray_distribution[40,78,:]\n",
    "\n",
    "# # # 绘制图像\n",
    "# # plt.figure()\n",
    "# # plt.plot(ray_distribution_one)\n",
    "# # plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c761361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据LOS_dir计算需要旋转的角度\n",
    "def calculate_rotation_angle(LOS_dir):\n",
    "    \"\"\"\n",
    "    计算LOS方向向量与X轴之间的旋转角度（顺时针）\n",
    "    \n",
    "    参数:\n",
    "        LOS_dir: 3D LOS方向向量 (numpy array)\n",
    "    \n",
    "    返回:\n",
    "        旋转角度（度）\n",
    "    \"\"\"\n",
    "    # 计算LOS方向向量在XZ平面上的投影\n",
    "    xz_projection = np.array([LOS_dir[0],0, LOS_dir[2]])\n",
    "    \n",
    "    # 计算投影与原向量之间的夹角\n",
    "    if np.linalg.norm(xz_projection) == 0:\n",
    "        return 0, 0  # 如果投影为零向量，返回0角度\n",
    "    xz_projection = xz_projection / np.linalg.norm(xz_projection)  # 归一化投影向量\n",
    "    LOS_dir = LOS_dir / np.linalg.norm(LOS_dir)  # 归一化LOS方向向量\n",
    "    # 计算投影与LOS方向向量的夹角\n",
    "    angle_rad1 = np.arccos(np.clip(np.dot(xz_projection, LOS_dir), -1.0, 1.0))\n",
    "    \n",
    "    angle_deg1 = np.degrees(angle_rad1)  # 转换为度数\n",
    "\n",
    "    # 计算投影的余弦值\n",
    "    angle_rad2 = np.arctan2(xz_projection[2], xz_projection[0])\n",
    "    angle_deg2 = np.degrees(angle_rad2)\n",
    "    \n",
    "    return angle_deg1, angle_deg2\n",
    "\n",
    "def merge_density_volumes_and_extract_mesh(images, LOS_dirs, omegas, model, \n",
    "                                          num_views=8, grid_size=120, threshold_ratio=0.3):\n",
    "    \"\"\"\n",
    "    通过体素融合多个视角的密度信息，然后提取统一网格\n",
    "    \n",
    "    参数:\n",
    "        images, LOS_dirs, omegas: 输入数据\n",
    "        model: NeRF模型\n",
    "        num_views: 融合的视角数量\n",
    "        grid_size: 体素网格大小\n",
    "        threshold_ratio: 最终提取网格的阈值\n",
    "    \"\"\"\n",
    "    # 创建3D体素网格\n",
    "    fused_density = np.zeros((grid_size, grid_size, grid_size))\n",
    "    weight_grid = np.zeros((grid_size, grid_size, grid_size))\n",
    "    \n",
    "    # 选择多个视角\n",
    "    selected_indices = random.sample(range(len(images)), min(num_views, len(images)))\n",
    "    \n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        print(f\"融合第 {i+1}/{num_views} 个视角...\")\n",
    "\n",
    "        omegas_batch_tensor,LOS_dirs_batch_tensor,range_profile_batch_tensor,doppler_profil_num_tensor = picture_sample(images,LOS_dirs,omegas,batch_size = 40,image_num=image_num,temp_num=[idx])\n",
    "        \n",
    "        # # 生成当前视角的密度分布\n",
    "        # omegas_batch = [omegas[idx]]\n",
    "        # LOS_dirs_batch = [LOS_dirs[idx]]\n",
    "        # doppler_numbers = [50]\n",
    "        \n",
    "        # omegas_batch_tensor = torch.stack(omegas_batch).to(device)\n",
    "        # LOS_dirs_batch_tensor = torch.stack(LOS_dirs_batch).to(device)\n",
    "        # doppler_profil_num_tensor = torch.tensor(doppler_numbers).long().to(device)\n",
    "        \n",
    "        # 渲染\n",
    "        distance_profile_batch, output = batchrender(\n",
    "            omegas_batch_tensor * omega_real,\n",
    "            LOS_dirs_batch_tensor,\n",
    "            model,\n",
    "            doppler_profil_num_tensor\n",
    "        )\n",
    "        \n",
    "        ray_distribution = output[0].detach().cpu().numpy()\n",
    "        \n",
    "        # 计算权重（基于视角质量或其他因素）\n",
    "        weight = 1.0  # 可以根据需要调整权重\n",
    "        \n",
    "        # 将当前密度添加到融合网格\n",
    "        # 这里需要将ray_distribution变换到统一的世界坐标系\n",
    "        LOS_dir = LOS_dirs_batch_tensor[0,:].cpu().numpy()\n",
    "        angle1, angle2 = calculate_rotation_angle(LOS_dir)\n",
    "        \n",
    "        # 变换密度网格到世界坐标\n",
    "        transformed_density = transform_density_to_world(ray_distribution, angle1, angle2, grid_size)\n",
    "        \n",
    "        # 融合\n",
    "        fused_density += transformed_density * weight\n",
    "        weight_grid += weight\n",
    "    \n",
    "    # 归一化\n",
    "    valid_mask = weight_grid > 0\n",
    "    fused_density[valid_mask] /= weight_grid[valid_mask]\n",
    "    \n",
    "    # 从融合密度提取网格\n",
    "    vmin, vmax = fused_density.min(), fused_density.max()\n",
    "    threshold = vmin + (vmax - vmin) * threshold_ratio\n",
    "    \n",
    "    verts, faces, normals, values = measure.marching_cubes(fused_density, threshold)\n",
    "    verts = verts / grid_size * 1.2 - 0.6\n",
    "    \n",
    "    return verts, faces, fused_density\n",
    "\n",
    "def transform_density_to_world(ray_distribution, angle1, angle2, target_size):\n",
    "    \"\"\"\n",
    "    将局部密度分布变换到世界坐标系\n",
    "    \"\"\"\n",
    "    # 这是一个简化实现，实际需要根据具体的坐标变换来实现\n",
    "    # 可能需要使用插值来重新采样到统一网格\n",
    "    from scipy.ndimage import zoom\n",
    "    \n",
    "    # 简单的尺寸调整\n",
    "    if ray_distribution.shape != (target_size, target_size, target_size):\n",
    "        scale_factors = [target_size / s for s in ray_distribution.shape]\n",
    "        transformed = zoom(ray_distribution, scale_factors)\n",
    "    else:\n",
    "        transformed = ray_distribution.copy()\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "\n",
    "print(\"开始体素融合...\")\n",
    "fused_vertices, fused_faces, fused_density = merge_density_volumes_and_extract_mesh(\n",
    "    images, LOS_dirs, omegas, model,\n",
    "    num_views=8, grid_size=120, threshold_ratio=0.3\n",
    ")\n",
    "\n",
    "# 保存融合模型\n",
    "if fused_vertices is not None:\n",
    "    fused_output_path = f'/DATA/disk1/asteroid/asteroid_inverse/Instant-ngp/model/{modelname}/fused_model.obj'\n",
    "    with open(fused_output_path, 'w') as f:\n",
    "        f.write(f\"# 体素融合模型，顶点数: {len(fused_vertices)}, 面数: {len(fused_faces)}\\n\\n\")\n",
    "        \n",
    "        for v in fused_vertices:\n",
    "            f.write(f\"v {v[0]} {v[1]} {v[2]}\\n\")\n",
    "        \n",
    "        for face in fused_faces:\n",
    "            f.write(f\"f {face[0]+1} {face[1]+1} {face[2]+1}\\n\")\n",
    "    \n",
    "    print(f\"融合模型已保存: {fused_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3d_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
